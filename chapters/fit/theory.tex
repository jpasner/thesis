\section{Bayes' Theorem and Limit Setting} \label{sec:fit:theory}

In the Bayesian approach, the data are considered to be fixed, and a
probability is assigned to the parameters representing the predictions of the
theory. Thus, given the data, a probability is assigned to the number of signal
events which could be present in the data.  In other words, Bayes' theorem
enables the calculation of the conditional probability of event $A$, theory
predictions, occurring given that event $B$, the data, has occurred. This is
given by 

\begin{equation} \label{sec:fit:general_bayes} 
P(A|B) = \frac{P(B|A)P(A)}{P(B)}, 
\end{equation}

where $P(B|A)$ is the conditional probability of event $B$ occurring, given that
event $A$ has occurred, $P(A)$ is the probability of event $A$ occurring, and
$P(B)$ is the probability of event $B$ occurring. For events where the outcome
is continuous rather than discrete, the probabilities ($P$) are replaced by
probability density functions ($p$ or p.d.f).

In this analysis the parameters of the theory are the normalization of the
signal template $\nu$, which is equivalent to the number of signal events, and
the nuisance parameters $\boldsymbol{\theta}$, which are the systematics
discussed in \Cref{chap:systematics}. Utilizing the continuous form of Bayes'
theorem given in \Cref{sec:fit:general_bayes}, the p.d.f can be calculated for
the theory parameters ($\nu,\boldsymbol{\theta}$), given the data.  This
particular p.d.f is referred to as the posterior, as it reflects the knowledge
about the theory parameters after the data has been analyzed; its formula is given in
\Cref{eq:fit:posterior}.

\begin{equation} \label{eq:fit:posterior}
p(\nu,\boldsymbol{\theta}|\text{data}) = \frac{\mathcal{L}(\nu,\boldsymbol{\theta}|\text{data})\pi(\nu,\boldsymbol{\theta})}{p(\text{data})}
\end{equation}

Here the likelihood of the theory parameters, given the data, is denoted
$\mathcal{L}(\nu,\boldsymbol{\theta}|\text{data})$ and is equivalent to
$p(\text{data}|\nu,\boldsymbol{\theta})$.  The exact form of the likelihood is
a choice of the analyzer and is discussed further in \Cref{sec:fit:bat}. The
information known about the theory parameters before the data is analyzed is
contained in the prior probability density $\pi(\nu,\boldsymbol{\theta})$.
This can be re-written as $\pi(\nu,\boldsymbol{\theta}) =
\pi(\nu)\prod_{i}\pi(\theta_i)$ since the parameter of interest $\nu$ and each
of the nuisance parameters $\theta_i$ are independent from each other.  The form
of the priors for the signal and each systematic is chosen by the analyzer and
is discussed in \Cref{sec:fit:priors}.  Note that the denominator of
\Cref{eq:fit:posterior}, $p(\text{data})$, is simply an overall normalization
factor and therefore can be dropped during the limit setting procedure.

The ultimate goal is to obtain the p.d.f for a single parameter, the number of
signal events $\nu$, given the data.  This is known as the marginalized
posterior $p(\nu|\text{data})$.  This is done by integrating out the dependence
of \Cref{eq:fit:posterior} on $\boldsymbol{\theta}$, as shown in
\Cref{eq:fit:marginalization}.

\begin{equation} \label{eq:fit:marginalization}
p(\nu|\text{data}) = \int p(\nu,\boldsymbol{\theta}|\text{data})\text{d}\boldsymbol{\theta}
\end{equation}

This multi-dimensional integral over the nuisance parameters $\boldsymbol{\theta}$ is known as
marginalization.  Note that the space of nuisance parameters may be very large,
making this integral extremely computationally expensive to calculate.  To
expedite this process a Markov Chain Monte Carlo procedure is implemented as
discussed in \Cref{sec:fit:bat}. By substituting the definition of the
posterior (\Cref{eq:fit:posterior}) into the marginalized posterior
(\Cref{eq:fit:marginalization}), utilizing the expanded prior probability
density $\pi(\nu,\boldsymbol{\theta}) = \pi(\nu)\prod_{i}\pi(\theta_i)$ and
dropping the $p(\text{data})$ normalization factor gives the following
definition of Bayes' equation: 

\begin{equation} \label{eq:fit:bayes}
p(\nu|\text{data}) \propto \int \mathcal{L}(\nu,\boldsymbol{\theta}|\text{data})\pi(\nu)\prod_{i}\pi(\theta_i)\text{d}\boldsymbol{\theta}
\end{equation}

This form of Bayes' equation can be interpreted as follows: the prior knowledge
of the analyzer encoded into $\pi(\nu)$ and $\pi(\theta_{i})$ is updated by
the outcome of the experiment contained in the likelihood
$\mathcal{L}(\nu,\boldsymbol{\theta}|\text{data})$ by marginalizing the
posterior with respect to $\boldsymbol{\theta}$ in order to obtain the p.d.f of the
parameter of interest given the data, $p(\nu|\text{data})$.  This is shown
diagrammatically for the simplified case (neglecting nuisance parameters) for a
small data sample in \Cref{fig:fit:small_data} and a large data sample in
\Cref{fig:fit:big_data}, both with a flat signal prior.  The flat prior in
these figures represents an uninformative prior that reflects ignorance about
the value of the parameter, so that it has minimal influence on the obtained
posterior.  This choice is consistent with the prior used in a similar dijet analysis
\cite{Beresford:2642397} and simplifies the comparison of results.

\begin{figure}[!htbp]
\centering
\subcaptionbox{\label{fig:fit:small_data}}{\includegraphics[width=0.49\linewidth]{figures/fit/small_data}} \hfill
\subcaptionbox{\label{fig:fit:big_data}}{\includegraphics[width=0.49\linewidth]{figures/fit/big_data}}
\caption{Cartoon representing the relationship between the signal normalization prior $\pi(\nu)$ (red), likelihood $\mathcal{L}(\nu|\text{data})$ (blue), and posterior $p(\nu|\text{data})$ (black) for the cases of (a) a small data sample and (b) a larger data sample.  This simplified example contains no nuisance parameters.}
\label{sec:fit:marginalization}
\end{figure}

By integrating the marginalized posterior $p(\nu|\text{data})$ across a range
of $\nu$ values, the probability that the true value of $\nu$ lies in the chosen
region can be determined.  A common choice in particle physics is to define the
upper limit $\nu_{\text{upper}}$ as the value below which 95\% of the marginalized
posterior lies, referred to as the 95\% credibility level (CL) as shown in
\Cref{eq:fit:limit} below.

\begin{equation} \label{eq:fit:limit}
0.95 = \int_{-\infty}^{\nu_{\text{upper}}} p(\nu|\text{data}) \text{d}\nu
\end{equation}

Here p(\nu|\text{data}) is given by \Cref{eq:fit:bayes}. This means there is a
95\% probability that the number of signal events in the data is less than or equal to
$\nu_{\text{upper}}$.  This definition is used in the final fit to
define the 95\% CL.


